---
layout: post
title: "Graph Graph Convolutional Networks"
date: 2018-08-30 13:44:01 -0500
tag: Graph data analysis
---
* content
{:toc}

Using nerual network at graph data.
The basic formula of this is thta:
    $$ H(x)^{(l+1)} = f(H^{(l)},A) $$
    $$ f(H(x)^{(l)},A) = \delta(AH^{(l)}W^{(l)}) $$
$ l $ is the layer of nerual network.

In [Thomas Kipf](https://tkipf.github.io/graph-convolutional-networks/) post, we
know that GCN works better than ordianry methods, even in three layers and
simple active function like ReLU function.

In order to get familiar with GCN, these paper should be finished before next
week, and then write short paper review:
- [Convolutional Networks on Graphs for Learning Molecular Fingerprints](http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf)
- [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/pdf/1609.02907.pdf)

And there other related paper focus on other areas:
- [Deep Walk](https://arxiv.org/pdf/1403.6652.pdf)
- [Trainable Adjancy Matrix](https://arxiv.org/pdf/1611.08402.pdf)

And currently, I still cannot understand why should the biggest eignvalues of
adjancy matrix should less than 1 to prove Asymptotic stability. And this
stablity often prove the system will keep in low activity. 

